import os
import joblib
import numpy as np
import pandas as pd
import multiprocessing
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, make_scorer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from .Data_Loader import create_text_container
class Hyperparameter_Grid_Search_Classification:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_file": ("STRING",),
                "algorithm": (["xgboost", "random_forest", "decision_tree", "lightgbm", "logistic", "lasso", "svm"],),
                "advanced": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                # ê³µí†µ íŒŒë¼ë¯¸í„°
                "target_column": ("STRING", {"default": "Label"}),
                "test_size": ("FLOAT", {"default": 0.2, "min": 0.1, "max": 0.5, "step": 0.05}),
                "num_cores": ("INT", {"default": 4, "min": 1, "max": 16, "step": 1}),
                "cv_splits": ("INT", {"default": 5, "min": 3, "max": 10, "step": 1}),
                "verbose": ("INT", {"default": 1, "min": 0, "max": 2, "step": 1}),
                "random_state": ("INT", {"default": 42, "min": 0, "max": 999, "step": 1}),
                
                # íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„° (XGBoost, Random Forest, LightGBM)
                "n_estimators": ("STRING", {"default": "[100, 200, 300]", "description": "tree number (XGBoost, Random Forest, LightGBM)"}),
                "max_depth": ("STRING", {"default": "[3, 5, 7]", "description": "tree max depth (XGBoost, Random Forest, Decision Tree, LightGBM)"}),
                "learning_rate": ("STRING", {"default": "[0.01, 0.05, 0.1]", "description": "learning rate (XGBoost, LightGBM)"}),
                
                # íŠ¸ë¦¬ ì„¸ë¶€ íŒŒë¼ë¯¸í„°
                "min_samples_split": ("STRING", {"default": "[2, 5, 10]", "description": "min samples split (Random Forest, Decision Tree)"}),
                "min_samples_leaf": ("STRING", {"default": "[1, 2, 4]", "description": "min samples leaf (Decision Tree)"}),
                "criterion": ("STRING", {"default": "['gini', 'entropy']", "description": "criterion (Decision Tree)"}),
                
                # LightGBM ì „ìš© íŒŒë¼ë¯¸í„°
                "subsample": ("STRING", {"default": "[0.6, 0.8, 1.0]", "description": "subsampling ratio (LightGBM)"}),
                "reg_alpha": ("STRING", {"default": "[0.1, 1, 10]", "description": "L1 regularization (LightGBM)"}),
                "reg_lambda": ("STRING", {"default": "[1, 10, 100]", "description": "L2 regularization (LightGBM)"}),
                
                # ì„ í˜• ëª¨ë¸ íŒŒë¼ë¯¸í„° (Logistic, Lasso, SVM)
                "C": ("STRING", {"default": "[0.01, 0.1, 1, 10, 100]", "description": "inverse of regularization strength (Logistic, Lasso, SVM)"}),
                "penalty": ("STRING", {"default": "['l2']", "description": "regularization type (Logistic)"}),
                "solver": ("STRING", {"default": "['lbfgs']", "description": "optimization algorithm (Logistic)"}),
                
                # SVM ì „ìš© íŒŒë¼ë¯¸í„°
                "kernel": ("STRING", {"default": "['linear', 'rbf']", "description": "kernel function (SVM)"}),
                "gamma": ("STRING", {"default": "['scale', 'auto']", "description": "kernel coefficient (SVM)"})
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING",)
    RETURN_NAMES = ("MODEL_PATH", "DESCRIPTORS_PATH", "X_TEST_PATH", "Y_TEST_PATH",)
    FUNCTION = "perform_grid_search"
    CATEGORY = "QSAR/CLASSIFICATION/GRID SEARCH"
    OUTPUT_NODE = True
    
    def perform_grid_search(self, input_file, algorithm, advanced,
                          test_size=0.2, num_cores=4, cv_splits=5, verbose=1, 
                          target_column="Label", random_state=42,
                          # ì•Œê³ ë¦¬ì¦˜ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°
                          n_estimators="[100, 200, 300]",
                          learning_rate="[0.01, 0.05, 0.1]",
                          max_depth="[3, 5, 7]",
                          min_samples_split="[2, 5, 10]",
                          min_samples_leaf="[1, 2, 4]",
                          criterion="['gini', 'entropy']",
                          subsample="[0.6, 0.8, 1.0]",
                          reg_alpha="[0.1, 1, 10]",
                          reg_lambda="[1, 10, 100]",
                          C="[0.01, 0.1, 1, 10, 100]",
                          penalty="['l2']",
                          solver="['lbfgs']",
                          kernel="['linear', 'rbf']",
                          gamma="['scale', 'auto']"):
        """
        Perform grid search for model hyperparameter optimization.
        """
        os.makedirs("QSAR/Model", exist_ok=True)
        
        # ë¬¸ìì—´ íŒŒë¼ë¯¸í„° íŒŒì‹± í—¬í¼ í•¨ìˆ˜
        def parse_param(param_str):
            try:
                # None ë¬¸ìì—´ ì²˜ë¦¬
                if "None" in param_str:
                    param_str = param_str.replace("None", "None")
                parsed = eval(param_str)
                return parsed
            except Exception as e:
                print(f"íŒŒë¼ë¯¸í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                if "None" in param_str:
                    return [None]
                return [0]

        # ë°ì´í„° ë¡œë“œ
        data = pd.read_csv(input_file)

        if target_column not in data.columns:
            raise ValueError(f"Target column '{target_column}' not found in dataset.")

        X = data.drop(columns=[target_column])
        y = data[target_column]

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,
                                                        random_state=random_state, stratify=y)

        # CPU ì½”ì–´ ìˆ˜ ì„¤ì •
        available_cores = max(1, multiprocessing.cpu_count())  
        num_cores = min(num_cores, available_cores)  

        # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì´ˆê¸°í™”
        param_grid = {}

        # ëª¨ë¸ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
        if algorithm == "xgboost":
            model = XGBClassifier(eval_metric="logloss", random_state=random_state)
            model_abbr = "XGB"
            # ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„° íŒŒì‹± - XGBoost ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'n_estimators': parse_param(n_estimators),
                'learning_rate': parse_param(learning_rate),
                'max_depth': parse_param(max_depth),
            }
            
        elif algorithm == "random_forest":
            model = RandomForestClassifier(random_state=random_state)
            model_abbr = "RF"
            # Random Forest ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'n_estimators': parse_param(n_estimators),
                'max_depth': parse_param(max_depth),
                'min_samples_split': parse_param(min_samples_split),
            }
            
        elif algorithm == "decision_tree":
            model = DecisionTreeClassifier(random_state=random_state)
            model_abbr = "DT"
            # Decision Tree ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'max_depth': parse_param(max_depth),
                'min_samples_split': parse_param(min_samples_split),
                'min_samples_leaf': parse_param(min_samples_leaf),
                'criterion': parse_param(criterion),
            }
            
        elif algorithm == "lightgbm":
            model = LGBMClassifier(random_state=random_state)
            model_abbr = "LGBM"
            # LightGBM ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'n_estimators': parse_param(n_estimators),
                'learning_rate': parse_param(learning_rate),
                'max_depth': parse_param(max_depth),
                'subsample': parse_param(subsample),
                'reg_alpha': parse_param(reg_alpha),
                'reg_lambda': parse_param(reg_lambda),
            }
            
        elif algorithm == "logistic":
            model = Pipeline([
                ("scaler", StandardScaler()),
                ("clf", LogisticRegression(max_iter=1000, random_state=random_state))
            ])
            model_abbr = "LogReg"
            # Logistic Regression ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'clf__C': parse_param(C),
                'clf__penalty': parse_param(penalty),
                'clf__solver': parse_param(solver),
            }
            
        elif algorithm == "lasso":
            model = Pipeline([
                ("scaler", StandardScaler()),
                ("clf", LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=random_state))
            ])
            model_abbr = "LASSO"
            # Lasso ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'clf__C': parse_param(C)
            }
            
        elif algorithm == "svm":
            model = Pipeline([
                ("scaler", StandardScaler()),
                ("clf", SVC(probability=True, random_state=random_state))
            ])
            model_abbr = "SVM"
            # SVM ê´€ë ¨ íŒŒë¼ë¯¸í„°
            param_grid = {
                'clf__C': parse_param(C),
                'clf__kernel': parse_param(kernel),
                'clf__gamma': parse_param(gamma),
            }
            
        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

        # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ í™•ì¸ ë° ë””ë²„ê¹…
        param_grid_str = "\n".join([f"{k}: {v}" for k, v in param_grid.items()])
        print(f"Algorithm: {algorithm}")
        print(f"Parameter Grid:\n{param_grid_str}")

        # í‰ê°€ ì§€í‘œ ì„¤ì •
        scoring = {
            'accuracy': 'accuracy',
            'f1': make_scorer(f1_score),
            'roc_auc': 'roc_auc',
            'precision': make_scorer(precision_score),
            'recall': make_scorer(recall_score),
        }

        # êµì°¨ ê²€ì¦ ì„¤ì •
        cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)
        
        # ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring=scoring,
                                refit='accuracy', return_train_score=True,
                                verbose=verbose, n_jobs=num_cores)

        grid_search.fit(X_train, y_train)

        # ìµœì  ëª¨ë¸ í‰ê°€
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_

        predictions = best_model.predict(X_test)
        eval_results = {
            "accuracy": accuracy_score(y_test, predictions),
            "f1_score": f1_score(y_test, predictions),
            "roc_auc": roc_auc_score(y_test, predictions),
            "precision": precision_score(y_test, predictions),
            "recall": recall_score(y_test, predictions),
        }

        # íŒŒì¼ ì €ì¥
        model_path = os.path.join("QSAR/Model", f"Best_Classifier_{model_abbr}.pkl")
        joblib.dump(best_model, model_path)

        descriptors_path = os.path.join("QSAR/Model", "Final_Selected_Descriptors.txt")
        with open(descriptors_path, "w") as f:
            f.write("\n".join(X_train.columns))

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
        X_test_path = os.path.join("QSAR/Model", "X_test.csv")
        y_test_path = os.path.join("QSAR/Model", "y_test.csv")
            
        X_test.to_csv(X_test_path, index=False)
        pd.DataFrame(y_test, columns=[target_column]).to_csv(y_test_path, index=False)

        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
        best_params_path = os.path.join("QSAR/Model", f"Best_Hyperparameters_{model_abbr}.txt")
        with open(best_params_path, "w") as f:
            for param, value in best_params.items():
                f.write(f"{param}: {value}\n")

        # ê²°ê³¼ ë¡œê·¸ ìƒì„±
        text_container = create_text_container(
            "ğŸ”¹ Classification Model Training Complete ğŸ”¹",
            f"ğŸ“Œ Best Algorithm: {algorithm}",
            f"ğŸ“Š Accuracy: {eval_results['accuracy']:.4f}",
            f"ğŸ“Š F1 Score: {eval_results['f1_score']:.4f}",
            f"ğŸ“Š ROC AUC: {eval_results['roc_auc']:.4f}",
            f"ğŸ“Š Precision: {eval_results['precision']:.4f}",
            f"ğŸ“Š Recall: {eval_results['recall']:.4f}",
            f"ğŸ”§ Best Parameters:" + "\n".join([f"  - {k}: {v}" for k, v in best_params.items()]),
        )

        return {
            "ui": {"text": text_container},
            "result": (str(model_path), str(descriptors_path), str(X_test_path), str(y_test_path),)
        }

# ë…¸ë“œ ë“±ë¡
NODE_CLASS_MAPPINGS = {
    "Hyperparameter_Grid_Search_Classification": Hyperparameter_Grid_Search_Classification
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Hyperparameter_Grid_Search_Classification": "Grid Search Hyperparameter (Classification)"
} 