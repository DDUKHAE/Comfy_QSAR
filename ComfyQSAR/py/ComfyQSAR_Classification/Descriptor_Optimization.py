import os
import numpy as np
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from .Descriptor_Preprocessing import create_text_container
class Remove_Low_Variance_Features_Classification:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_file": ("STRING",),
                "threshold": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.01})
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("OUTPUT_FILE",)
    FUNCTION = "remove_low_variance"
    CATEGORY = "QSAR/CLASSIFICATION/OPTIMIZATION"
    OUTPUT_NODE = True
    
    def remove_low_variance(self, input_file, threshold=0.05):
        """
        Remove low variance features from a dataset.
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("QSAR/Optimization", exist_ok=True)

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(input_file)

        # Label ì—´ ë¶„ë¦¬
        if "Label" not in df.columns:
            raise ValueError("The dataset must contain a 'Label' column.")

        target_column = df["Label"]
        feature_columns = df.drop(columns=["Label"])
        
        # ë¬´í•œê°’ ë° í° ê°’ ì²˜ë¦¬
        # inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ í›„ NaN ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
        feature_columns = feature_columns.replace([np.inf, -np.inf], np.nan)
        for col in feature_columns.columns:
            if feature_columns[col].isnull().any():
                median_val = feature_columns[col].median()
                feature_columns[col] = feature_columns[col].fillna(median_val)

        # ì €ë¶„ì‚° íŠ¹ì„± ì œê±°
        selector = VarianceThreshold(threshold=threshold)
        selected_features = selector.fit_transform(feature_columns)
        
        # ë‚¨ì€ ì—´ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        retained_columns = feature_columns.columns[selector.get_support()]

        # ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ìƒˆ DataFrame ìƒì„±
        df_retained = pd.DataFrame(selected_features, columns=retained_columns)
        df_retained["Label"] = target_column

        # íŒŒì¼ëª… ë™ì  ìƒì„±
        initial_count = feature_columns.shape[1]
        final_count = len(retained_columns)
        output_file = os.path.join("QSAR/Optimization", f"low_variance_results_({initial_count}_{final_count}).csv")
        
        # ì €ì¥
        df_retained.to_csv(output_file, index=False)

        # ë¡œê·¸ ë©”ì‹œì§€
        text_container = create_text_container(
            "ğŸ”¹ Low Variance Features Removed! ğŸ”¹",
            f"ğŸ“Š Initial Features: {initial_count}",
            f"ğŸ“‰ Remaining Features: {final_count}",
            f"ğŸ—‘ï¸ Removed: {initial_count - final_count}",
        )

        return {
            "ui": {"text": text_container},
            "result": (str(output_file),)
        }

class Remove_High_Correlation_Features_Classification:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_file": ("STRING",),
                "threshold": ("FLOAT", {"default": 0.95, "min": 0.5, "max": 1.0, "step": 0.01}),
                "mode": (["target_based", "upper", "lower"],),
                "importance_model": (["lasso", "random_forest"],)
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("OUTPUT_FILE",)
    FUNCTION = "remove_high_correlation"
    CATEGORY = "QSAR/CLASSIFICATION/OPTIMIZATION"
    OUTPUT_NODE = True
    
    def remove_high_correlation(self, input_file, threshold=0.95, mode="target_based", importance_model="lasso"):
        """
        Remove highly correlated features from a classification dataset while preserving the most informative ones.
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("QSAR/Optimization", exist_ok=True)
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(input_file)

        # Label ì—´ ì²´í¬
        if "Label" not in df.columns:
            raise ValueError("The dataset must contain a 'Label' column.")

        # ë¼ë²¨ ë° ì´ë¦„ ì—´ ë¶„ë¦¬
        target_column = df["Label"]
        feature_columns = df.drop(columns=["Label"])
        
        # ë¬´í•œê°’ ë° í° ê°’ ì²˜ë¦¬
        # inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ í›„ NaN ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
        feature_columns = feature_columns.replace([np.inf, -np.inf], np.nan)
        for col in feature_columns.columns:
            if feature_columns[col].isnull().any():
                median_val = feature_columns[col].median()
                feature_columns[col] = feature_columns[col].fillna(median_val)

        # ìƒê´€ í–‰ë ¬ ê³„ì‚°
        correlation_matrix = feature_columns.corr()
        to_remove = set()

        if mode == "target_based":
            # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
            feature_target_corr = feature_columns.corrwith(target_column).abs()
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° (ì§€ì •ëœ ê²½ìš°)
            feature_importance = {}

            if importance_model in ["lasso", "random_forest"]:
                X, y = feature_columns, target_column

                if importance_model == "lasso":
                    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì „ì²˜ë¦¬ ì¶”ê°€
                    scaler = StandardScaler()
                    try:
                        # ì¶”ê°€ ì „ì²˜ë¦¬: ë¬´í•œê°’ê³¼ NaNì„ ì œê±°í•œ í›„ ìŠ¤ì¼€ì¼ë§
                        X_copy = X.copy()
                        X_copy = X_copy.replace([np.inf, -np.inf], np.nan)
                        for col in X_copy.columns:
                            if X_copy[col].isnull().any():
                                median_val = X_copy[col].median()
                                X_copy[col] = X_copy[col].fillna(median_val)
                        
                        X_scaled = scaler.fit_transform(X_copy)
                        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
                        
                        # 1ë‹¨ê³„: ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì‹œë„
                        print("LASSO 1ë‹¨ê³„: ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ ì‹œë„...")
                        model = Lasso(random_state=42)
                        model.fit(X_scaled_df, y)
                        importance_values = np.abs(model.coef_)
                        print("âœ… LASSO 1ë‹¨ê³„ í•™ìŠµ ì„±ê³µ!")
                    except Exception as e:
                        print(f"LASSO 1ë‹¨ê³„ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        try:
                            # 2ë‹¨ê³„: alpha 0.1, max_iter 10,000ìœ¼ë¡œ ì‹œë„
                            print("LASSO 2ë‹¨ê³„: alpha=0.1, max_iter=10,000ìœ¼ë¡œ í•™ìŠµ ì‹œë„...")
                            model = Lasso(alpha=0.1, max_iter=10000, random_state=42)
                            model.fit(X_scaled_df, y)
                            importance_values = np.abs(model.coef_)
                            print("âœ… LASSO 2ë‹¨ê³„ í•™ìŠµ ì„±ê³µ!")
                        except Exception as e2:
                            print(f"LASSO 2ë‹¨ê³„ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e2}")
                            try:
                                # 3ë‹¨ê³„: alpha 1.0, max_iter 20,000ìœ¼ë¡œ ì‹œë„
                                print("LASSO 3ë‹¨ê³„: alpha=1.0, max_iter=20,000ìœ¼ë¡œ í•™ìŠµ ì‹œë„...")
                                model = Lasso(alpha=1.0, max_iter=20000, random_state=42)
                                model.fit(X_scaled_df, y)
                                importance_values = np.abs(model.coef_)
                                print("âœ… LASSO 3ë‹¨ê³„ í•™ìŠµ ì„±ê³µ!")
                            except Exception as e3:
                                print(f"LASSO 3ë‹¨ê³„ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e3}")
                                # ì˜¤ë¥˜ ë°œìƒ ì‹œ RandomForestë¡œ ëŒ€ì²´
                                try:
                                    print("ëŒ€ì²´ ëª¨ë¸: RandomForest ì‚¬ìš©...")
                                    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
                                    model.fit(X, y)
                                    importance_values = model.feature_importances_
                                    print("âœ… RandomForest ëŒ€ì²´ ëª¨ë¸ í•™ìŠµ ì„±ê³µ!")
                                except Exception as e4:
                                    print(f"RandomForest ëŒ€ì²´ ëª¨ë¸ë„ ì‹¤íŒ¨: {e4}")
                                    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ ë™ì¼í•œ ì¤‘ìš”ë„ ë¶€ì—¬
                                    print("âš ï¸ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨. ë™ì¼í•œ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                    importance_values = np.ones(X.shape[1]) / X.shape[1]
                else:
                    # RandomForestì˜ ê²½ìš° ë²”ìœ„ê°€ í¬ê±°ë‚˜ ë¬´í•œì¸ ê°’ì„ ì²˜ë¦¬
                    model = RandomForestClassifier(n_estimators=200, random_state=42)
                    try:
                        model.fit(X, y)
                        importance_values = model.feature_importances_
                    except Exception as e:
                        print(f"RandomForest ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        # ì¶”ê°€ì ì¸ ìŠ¤ì¼€ì¼ë§ ì‹œë„
                        try:
                            scaler = StandardScaler()
                            X_scaled = scaler.fit_transform(X)
                            X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
                            model.fit(X_scaled_df, y)
                            importance_values = model.feature_importances_
                        except Exception as e2:
                            print(f"ìŠ¤ì¼€ì¼ë§ í›„ì—ë„ ì˜¤ë¥˜ ë°œìƒ: {e2}")
                            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë“  íŠ¹ì„±ì— ë™ì¼í•œ ì¤‘ìš”ë„ ë¶€ì—¬
                            importance_values = np.ones(X.shape[1]) / X.shape[1]
                
                feature_importance = dict(zip(feature_columns.columns, importance_values))

                
            # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ìŒ ì°¾ê¸°
            rows, cols = np.where(np.abs(np.triu(correlation_matrix, k=1)) > threshold)
            for row, col in zip(rows, cols):
                f1 = correlation_matrix.columns[row]
                f2 = correlation_matrix.columns[col]

                # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë¹„êµ
                if feature_target_corr[f1] > feature_target_corr[f2]:
                    weaker = f2
                elif feature_target_corr[f1] < feature_target_corr[f2]:
                    weaker = f1
                else:
                    # ê°™ì€ ê²½ìš° íŠ¹ì„± ì¤‘ìš”ë„ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
                    weaker = f2 if feature_importance.get(f1, 0) > feature_importance.get(f2, 0) else f1

                to_remove.add(weaker)

        else:
            # "upper" ë˜ëŠ” "lower" ëª¨ë“œ ì‚¬ìš©
            tri = np.triu(correlation_matrix, k=1) if mode == "upper" else np.tril(correlation_matrix, k=-1)
            rows, cols = np.where(np.abs(tri) > threshold)
            for row, col in zip(rows, cols):
                f1 = correlation_matrix.columns[row]
                f2 = correlation_matrix.columns[col]
                to_remove.add(f2 if mode == "upper" else f1)

        # ì œê±°ë˜ì§€ ì•Šì€ ì—´ë§Œ ìœ ì§€
        retained_columns = [c for c in feature_columns.columns if c not in to_remove]
        df_retained = feature_columns[retained_columns]
        df_retained["Label"] = target_column

        # íŒŒì¼ëª… ë™ì  ìƒì„±
        initial_count = feature_columns.shape[1]
        final_count = len(retained_columns)
        output_file = os.path.join("QSAR/Optimization", f"high_correlation_results_({initial_count}_{final_count}).csv")
        
        # ì €ì¥
        df_retained.to_csv(output_file, index=False)

        # ë¡œê·¸ ë©”ì‹œì§€
        text_container = create_text_container(
            "ğŸ”¹ High Correlation Features Removed! ğŸ”¹",
            f"ğŸ“Š Initial Features: {initial_count}",
            f"ğŸ“‰ Remaining Features: {final_count}",
            f"ğŸ—‘ï¸ Removed: {initial_count - final_count}",
            f"ğŸ”§ Mode: {mode}, Model: {importance_model if mode=='target_based' else 'N/A'}",
        )

        return {
            "ui": {"text": text_container},
            "result": (str(output_file),)
        }

class Descriptor_Optimization_Classification:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_file": ("STRING",),
                "variance_threshold": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.01}),
                "correlation_threshold": ("FLOAT", {"default": 0.95, "min": 0.5, "max": 1.0, "step": 0.01}),
                "correlation_mode": (["target_based", "upper", "lower"],),
                "importance_model": (["lasso", "random_forest"],)
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("OUTPUT_FILE",)
    FUNCTION = "optimize_descriptors"
    CATEGORY = "QSAR/CLASSIFICATION/OPTIMIZATION"
    OUTPUT_NODE = True
    
    def optimize_descriptors(self, input_file, variance_threshold=0.05, 
                             correlation_threshold=0.95, correlation_mode="target_based", 
                             importance_model="lasso"):
        """
        Complete descriptor optimization pipeline:
        1. Remove low variance features
        2. Remove highly correlated features
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("QSAR/Optimization", exist_ok=True)
        
        # ë°ì´í„°ì— inf ê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
        try:
            df = pd.read_csv(input_file)
            if "Label" in df.columns:
                target_column = df["Label"].copy()
                feature_columns = df.drop(columns=["Label"])
                
                # inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ í›„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                feature_columns = feature_columns.replace([np.inf, -np.inf], np.nan)
                for col in feature_columns.columns:
                    if feature_columns[col].isnull().any():
                        median_val = feature_columns[col].median()
                        feature_columns[col] = feature_columns[col].fillna(median_val)
                
                # ì²˜ë¦¬ëœ ë°ì´í„° ë‹¤ì‹œ ì €ì¥
                processed_df = feature_columns.copy()
                processed_df["Label"] = target_column
                
                temp_file = os.path.join("QSAR/Optimization", "temp_preprocessed.csv")
                processed_df.to_csv(temp_file, index=False)
                
                # ì „ì²˜ë¦¬ëœ íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                input_file = temp_file
        except Exception as e:
            print(f"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ íŒŒì¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # 1. ì €ë¶„ì‚° íŠ¹ì„± ì œê±°
        variance_remover = Remove_Low_Variance_Features_Classification()
        variance_result = variance_remover.remove_low_variance(input_file, threshold=variance_threshold)
        variance_output = variance_result["result"][0]
        
        # 2. ê³ ìƒê´€ íŠ¹ì„± ì œê±°
        correlation_remover = Remove_High_Correlation_Features_Classification()
        correlation_result = correlation_remover.remove_high_correlation(
            variance_output, threshold=correlation_threshold, 
            mode=correlation_mode, importance_model=importance_model
        )
        final_output = correlation_result["result"][0]
        
        # ìµœì¢… ë°ì´í„° ë¡œë“œ
        final_data = pd.read_csv(final_output)
        
        # ì´ˆê¸° ë°ì´í„° ë¡œë“œ (í†µê³„ ë¹„êµìš©)
        initial_data = pd.read_csv(input_file)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(os.path.join("QSAR/Optimization", "temp_preprocessed.csv")):
            try:
                os.remove(os.path.join("QSAR/Optimization", "temp_preprocessed.csv"))
            except:
                pass
        
        # íŠ¹ì„± ìˆ˜ ê³„ì‚°
        initial_features = initial_data.shape[1] - (1 if "Label" in initial_data.columns else 0) - (1 if "Name" in initial_data.columns else 0)
        final_features = final_data.shape[1] - (1 if "Label" in final_data.columns else 0) - (1 if "Name" in final_data.columns else 0)
        
        # ë¡œê·¸ ë©”ì‹œì§€
        text_container = create_text_container(
            "ğŸ”¹ Complete Descriptor Optimization Done! ğŸ”¹",
            f"ğŸ“Š Initial Features: {initial_features}",
            f"ğŸ“‰ Final Features: {final_features}",
            f"ğŸ—‘ï¸ Total Removed: {initial_features - final_features} ({(initial_features - final_features) / initial_features * 100:.1f}%)",
            f"ğŸ”§ Optimization Pipeline:",
            f"   1. Removed low variance features (threshold: {variance_threshold})",
            f"   2. Removed highly correlated features (threshold: {correlation_threshold})",
            f"      Mode: {correlation_mode}, Model: {importance_model if correlation_mode=='target_based' else 'N/A'}",
            )

        return {
            "ui": {"text": text_container},
            "result": (str(final_output),)
        }

# ë…¸ë“œ ë“±ë¡
NODE_CLASS_MAPPINGS = {
    "Remove_Low_Variance_Features_Classification": Remove_Low_Variance_Features_Classification,
    "Remove_High_Correlation_Features_Classification": Remove_High_Correlation_Features_Classification,
    "Descriptor_Optimization_Classification": Descriptor_Optimization_Classification
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Remove_Low_Variance_Features_Classification": "Remove Low Variance Features(Classification)",
    "Remove_High_Correlation_Features_Classification": "Remove High Correlation Features(Classification)",
    "Descriptor_Optimization_Classification": "Descriptor Optimization(Classification)"
} 