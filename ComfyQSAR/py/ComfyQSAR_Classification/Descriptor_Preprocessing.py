import os
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
# from .Data_Loader import create_text_container # Now imported from progress_utils

# --- Common Utility Import ---
from server import PromptServer
import time # time ëª¨ë“ˆ ì¶”ê°€

# WebSocket ì´ë²¤íŠ¸ ì´ë¦„ ì •ì˜ (ëª¨ë“  QSAR ë…¸ë“œì—ì„œ ê³µí†µ ì‚¬ìš©)
QSAR_PROGRESS_EVENT = "qsar-desc-calc-progress" # ì´ë¦„ì„ ì¢€ ë” ë²”ìš©ì ìœ¼ë¡œ ë³€ê²½ (ì„ íƒì )
# ë˜ëŠ” ê¸°ì¡´ ì´ë¦„ ìœ ì§€: QSAR_DESC_CALC_PROGRESS_EVENT = "qsar-desc-calc-progress"

def send_progress(message, progress=None, node_id=None):
    """
    ì§€ì •ëœ ë©”ì‹œì§€ì™€ ì§„í–‰ë¥ (0-100)ì„ WebSocketì„ í†µí•´ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡í•˜ê³ ,
    ì¤‘ê°„ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ì‹œ ì§§ì€ ì§€ì—° ì‹œê°„ì„ ì¶”ê°€í•˜ì—¬ UIì—ì„œ ë³¼ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    Args:
        message (str): í‘œì‹œí•  ìƒíƒœ ë©”ì‹œì§€.
        progress (Optional[float]): 0ë¶€í„° 100 ì‚¬ì´ì˜ ì§„í–‰ë¥  ê°’.
        node_id (Optional[str]): íŠ¹ì • ë…¸ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ í•  ê²½ìš° ë…¸ë“œ ID.
    """
    payload = {"text": [message]}
    is_intermediate_update = False # ì¤‘ê°„ ì—…ë°ì´íŠ¸ ì—¬ë¶€ í”Œë˜ê·¸

    if progress is not None:
        # ì§„í–‰ë¥  ê°’ì„ 0ê³¼ 100 ì‚¬ì´ë¡œ ì œí•œí•˜ê³  ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼ (ì„ íƒì )
        clamped_progress = max(0.0, min(100.0, float(progress)))
        payload['progress'] = round(clamped_progress, 1)
        # 100%ê°€ ì•„ë‹Œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ì¸ì§€ í™•ì¸
        if clamped_progress < 100:
            is_intermediate_update = True

    # node ID ì¶”ê°€ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í•„í„°ë§ ì‹œ ì‚¬ìš© ê°€ëŠ¥)
    # if node_id: payload['node'] = node_id

    try:
        # PromptServer ì¸ìŠ¤í„´ìŠ¤ë¥¼ í†µí•´ ë™ê¸°ì ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
        PromptServer.instance.send_sync(QSAR_PROGRESS_EVENT, payload)

        # ì¤‘ê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í›„ ì§§ì€ ì§€ì—° ì‹œê°„ ì¶”ê°€ (0.2ì´ˆ)
        # ìµœì¢…(100%) ì—…ë°ì´íŠ¸ ì‹œì—ëŠ” ì§€ì—° ì—†ìŒ
        if is_intermediate_update:
            time.sleep(0.2) # 0.2ì´ˆ ëŒ€ê¸°

    except Exception as e:
        print(f"[ComfyQSAR Progress Util] WebSocket ì „ì†¡ ì˜¤ë¥˜: {e}")

# í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ê°€ ê°€ëŠ¥ (ì˜ˆ: ì‹œê°„ í¬ë§·íŒ… ë“±) 

# í…ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ìƒì„± í—¬í¼ í•¨ìˆ˜
def create_text_container(*lines):
    # ê°€ì¥ ê¸´ ë¼ì¸ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„ì„  ê¸¸ì´ ê²°ì •
    max_length = max(len(line) for line in lines)
    separator = "=" * max_length
    
    # ì²« êµ¬ë¶„ì„  ì¶”ê°€
    result = [separator]
    
    # ê° ë¼ì¸ ì¶”ê°€
    for line in lines:
        result.append(line)
    
    # ë§ˆì§€ë§‰ êµ¬ë¶„ì„  ì¶”ê°€
    result.append(separator)
    
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¡°ì¸
    return "\n".join(result)

class Replace_inf_with_nan_Classification():
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_file": ("STRING",),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("CLEANED_DATA_PATH",) # More specific name
    FUNCTION = "replace_inf_with_nan"
    CATEGORY = "QSAR/CLASSIFICATION/PREPROCESSING"
    OUTPUT_NODE = True

    @staticmethod
    def replace_inf_with_nan(input_file):
        send_progress("ğŸš€ Starting Inf value replacement...", 0)
        output_dir = "QSAR/Descriptor_Preprocessing"
        inf_file = None # Initialize inf_file path
        output_file = "" # Initialize output_file path

        try:
            os.makedirs(output_dir, exist_ok=True)
            send_progress(f"ğŸ“‚ Output directory checked/created: {output_dir}", 5)

            send_progress(f"â³ Loading data from: {input_file}", 10)
            data = pd.read_csv(input_file)
            send_progress("   Data loaded.", 15)

            send_progress("âš™ï¸ Identifying numeric columns and checking for Inf values...", 20)
            numeric_df = data.select_dtypes(include=[np.number])
            inf_mask = numeric_df.isin([np.inf, -np.inf])
            inf_columns = numeric_df.columns[inf_mask.any(axis=0)].tolist()
            total_inf_count = inf_mask.sum().sum()
            send_progress(f"   Found {len(inf_columns)} columns with Inf values. Total Inf count: {total_inf_count}", 30)

            if total_inf_count > 0:
                send_progress("ğŸ› ï¸ Replacing Inf values with NaN...", 40)
                data.replace([np.inf, -np.inf], np.nan, inplace=True)
                send_progress("   Inf replacement complete.", 50)

                if inf_columns:
                    send_progress("ğŸ“Š Generating Inf report...", 60)
                    inf_counts = numeric_df[inf_columns].isin([np.inf, -np.inf]).sum().reset_index()
                    inf_counts.columns = ["Feature", "Original_Inf_Count"]
                    inf_counts = inf_counts[inf_counts["Original_Inf_Count"] > 0] # Only report columns that had inf
                    inf_file = os.path.join(output_dir, "inf_features_report.csv")
                    inf_counts.to_csv(inf_file, index=False)
                    send_progress(f"   Inf report saved to: {inf_file}", 70)
            else:
                 send_progress("âœ… No Inf values detected.", 50)


            send_progress("ğŸ’¾ Saving cleaned data...", 80)
            output_file = os.path.join(output_dir, "inf_replaced_data.csv") # More specific name
            data.to_csv(output_file, index=False)
            send_progress(f"   Cleaned data saved to: {output_file}", 85)

            send_progress("ğŸ“ Generating summary...", 95)
            text_container_content = create_text_container(
                "ğŸ”¹ **Inf Value Replacement Completed!** ğŸ”¹",
                f"Input File: {os.path.basename(input_file)}",
                f"Columns with Inf: {len(inf_columns)}",
                f"Total Inf Values Replaced: {total_inf_count}",
                f"Inf Report Saved: {inf_file}" if inf_file else "Inf Report: Not generated (no Inf values found).",
                f"Output File: {output_file}"
            )
            send_progress("ğŸ‰ Inf replacement process finished.", 100)

            return {"ui": {"text": text_container_content},
                    "result": (str(output_file),)}

        except FileNotFoundError as fnf_e:
            error_msg = f"âŒ File Not Found Error: {str(fnf_e)}. Please check input file path."
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except Exception as e:
            error_msg = f"âŒ An unexpected error occurred during Inf replacement: {str(e)}"
            import traceback
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}


class Remove_high_nan_compounds_Classification():
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_file": ("STRING",),
                "threshold": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01, "display": "number"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("COMPOUND_FILTERED_PATH",) # More specific name
    FUNCTION = "remove_high_nan_compounds"
    CATEGORY = "QSAR/CLASSIFICATION/PREPROCESSING"
    OUTPUT_NODE = True

    @staticmethod
    def remove_high_nan_compounds(input_file, threshold):
        send_progress("ğŸš€ Starting removal of compounds with high NaN ratio...", 0)
        output_dir = "QSAR/Descriptor_Preprocessing"
        output_file = ""

        try:
            os.makedirs(output_dir, exist_ok=True)
            send_progress(f"ğŸ“‚ Output directory checked/created: {output_dir}", 5)

            send_progress(f"â³ Loading data from: {input_file}", 10)
            data = pd.read_csv(input_file)
            original_rows, original_cols = data.shape
            send_progress(f"   Data loaded ({original_rows} rows, {original_cols} columns).", 15)

            send_progress(f"âš™ï¸ Calculating NaN percentage per compound (row)... Threshold: {threshold:.2f}", 20)
            nan_counts = data.isna().sum(axis=1)
            nan_percentage = nan_counts / original_cols # Use original_cols for percentage calculation
            send_progress("   NaN percentage calculation complete.", 30)

            send_progress(f"âœ‚ï¸ Filtering compounds with NaN ratio <= {threshold:.2f}...", 40)
            filtered_data = data[nan_percentage <= threshold].copy() # Use copy()
            filtered_rows = filtered_data.shape[0]
            removed_count = original_rows - filtered_rows
            send_progress(f"   Filtering complete. Kept {filtered_rows} compounds, removed {removed_count}.", 60)

            send_progress("ğŸ’¾ Saving filtered data...", 80)
            # Include original and filtered counts in filename
            output_file = os.path.join(output_dir, f"filtered_compounds_nan_{original_rows}_to_{filtered_rows}.csv")
            filtered_data.to_csv(output_file, index=False)
            send_progress(f"   Filtered data saved to: {output_file}", 85)

            send_progress("ğŸ“ Generating summary...", 95)
            text_container_content = create_text_container(
                "ğŸ”¹ **High NaN Compound Removal Completed!** ğŸ”¹",
                f"Input File: {os.path.basename(input_file)} ({original_rows} rows)",
                f"NaN Threshold: {threshold*100:.0f}% per compound",
                f"Compounds Retained: {filtered_rows}",
                f"Compounds Removed: {removed_count}",
                f"Output File: {output_file}"
            )
            send_progress("ğŸ‰ High NaN compound removal process finished.", 100)

            return {"ui": {"text": text_container_content},
                    "result": (str(output_file),)}

        except FileNotFoundError as fnf_e:
            error_msg = f"âŒ File Not Found Error: {str(fnf_e)}. Please check input file path."
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except Exception as e:
            error_msg = f"âŒ An unexpected error occurred during high NaN compound removal: {str(e)}"
            import traceback
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}


class Remove_high_nan_descriptors_Classification():
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_file": ("STRING",),
                "threshold": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01, "display": "number"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("DESCRIPTOR_FILTERED_PATH",) # More specific name
    FUNCTION = "remove_high_nan_descriptors"
    CATEGORY = "QSAR/CLASSIFICATION/PREPROCESSING"
    OUTPUT_NODE = True

    @staticmethod
    def remove_high_nan_descriptors(input_file, threshold):
        send_progress("ğŸš€ Starting removal of descriptors with high NaN ratio...", 0)
        output_dir = "QSAR/Descriptor_Preprocessing"
        output_file = ""

        try:
            os.makedirs(output_dir, exist_ok=True)
            send_progress(f"ğŸ“‚ Output directory checked/created: {output_dir}", 5)

            send_progress(f"â³ Loading data from: {input_file}", 10)
            data = pd.read_csv(input_file)
            original_rows, original_cols = data.shape
            send_progress(f"   Data loaded ({original_rows} rows, {original_cols} columns).", 15)

            send_progress(f"âš™ï¸ Calculating NaN percentage per descriptor (column)... Threshold: {threshold:.2f}", 20)
            nan_percentage = data.isna().mean() # Calculates mean (which is percentage for boolean mask)
            send_progress("   NaN percentage calculation complete.", 30)

            send_progress(f"âœ‚ï¸ Filtering descriptors with NaN ratio <= {threshold:.2f}...", 40)
            retained_columns = nan_percentage[nan_percentage <= threshold].index.tolist()

            # Ensure essential columns like 'Label' (or maybe 'SMILES', 'Name' if present) are kept
            essential_cols = ["Label", "SMILES", "Name"] # Add other potential identifiers
            for col in essential_cols:
                 if col in data.columns and col not in retained_columns:
                      retained_columns.append(col)
                      send_progress(f"   Ensured essential column '{col}' is retained.", 45)


            filtered_data = data[retained_columns].copy() # Use .copy()
            filtered_cols_count = filtered_data.shape[1]
            removed_count = original_cols - filtered_cols_count
            send_progress(f"   Filtering complete. Kept {filtered_cols_count} descriptors, removed {removed_count}.", 60)

            send_progress("ğŸ’¾ Saving filtered data...", 80)
            # Include original and filtered counts in filename
            output_file = os.path.join(output_dir, f"filtered_descriptors_nan_{original_cols}_to_{filtered_cols_count}.csv")
            filtered_data.to_csv(output_file, index=False)
            send_progress(f"   Filtered data saved to: {output_file}", 85)

            send_progress("ğŸ“ Generating summary...", 95)
            text_container_content = create_text_container(
                "ğŸ”¹ **High NaN Descriptor Removal Completed!** ğŸ”¹",
                f"Input File: {os.path.basename(input_file)} ({original_cols} columns)",
                f"NaN Threshold: {threshold*100:.0f}% per descriptor",
                f"Descriptors Retained: {filtered_cols_count}",
                f"Descriptors Removed: {removed_count}",
                f"Output File: {output_file}"
            )
            send_progress("ğŸ‰ High NaN descriptor removal process finished.", 100)

            return {"ui": {"text": text_container_content},
                    "result": (str(output_file),)}

        except FileNotFoundError as fnf_e:
            error_msg = f"âŒ File Not Found Error: {str(fnf_e)}. Please check input file path."
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except Exception as e:
            error_msg = f"âŒ An unexpected error occurred during high NaN descriptor removal: {str(e)}"
            import traceback
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}


class Impute_missing_values_Classification():
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_file": ("STRING",),
                "method": (["mean", "median", "most_frequent"], {"default": "mean"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("IMPUTED_DATA_PATH",) # More specific name
    FUNCTION = "impute_missing_values"
    CATEGORY = "QSAR/CLASSIFICATION/PREPROCESSING"
    OUTPUT_NODE = True

    @staticmethod
    def impute_missing_values(input_file, method):
        send_progress(f"ğŸš€ Starting missing value imputation using '{method}' strategy...", 0)
        output_dir = "QSAR/Descriptor_Preprocessing"
        output_file = ""

        try:
            os.makedirs(output_dir, exist_ok=True)
            send_progress(f"ğŸ“‚ Output directory checked/created: {output_dir}", 5)

            send_progress(f"â³ Loading data from: {input_file}", 10)
            data = pd.read_csv(input_file)
            original_rows, original_cols = data.shape
            send_progress(f"   Data loaded ({original_rows} rows, {original_cols} columns).", 15)

            # Store non-numeric columns (like Name, SMILES, Label) to add back later
            send_progress("âš™ï¸ Separating non-numeric columns...", 20)
            non_numeric_cols = data.select_dtypes(exclude=[np.number]).columns.tolist()
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

            if not numeric_cols:
                 raise ValueError("No numeric columns found in the input file to impute.")

            non_numeric_data = data[non_numeric_cols]
            numeric_data = data[numeric_cols]
            initial_nan_count = numeric_data.isna().sum().sum()
            send_progress(f"   Identified {len(numeric_cols)} numeric columns with {initial_nan_count} total missing values.", 25)


            send_progress(f"ğŸ› ï¸ Applying '{method}' imputation...", 30)
            imputer = SimpleImputer(strategy=method)
            imputed_numeric_data = pd.DataFrame(imputer.fit_transform(numeric_data), columns=numeric_cols, index=data.index) # Preserve index
            final_nan_count = imputed_numeric_data.isna().sum().sum() # Should be 0
            send_progress(f"   Imputation complete. Remaining NaNs in numeric columns: {final_nan_count}", 70)


            send_progress("âš™ï¸ Recombining imputed numeric data with non-numeric columns...", 75)
            # Concatenate based on index
            final_data = pd.concat([non_numeric_data, imputed_numeric_data], axis=1)
            # Ensure original column order if important (optional)
            final_data = final_data[data.columns]
            send_progress("   Data recombined.", 80)


            send_progress("ğŸ’¾ Saving imputed data...", 85)
            output_file = os.path.join(output_dir, f"imputed_{method}_data.csv") # Include method in name
            final_data.to_csv(output_file, index=False)
            send_progress(f"   Imputed data saved to: {output_file}", 90)

            send_progress("ğŸ“ Generating summary...", 95)
            text_container_content = create_text_container(
                "ğŸ”¹ **Missing Value Imputation Completed!** ğŸ”¹",
                f"Input File: {os.path.basename(input_file)} ({original_rows} rows, {original_cols} columns)",
                f"Imputation Strategy: '{method}'",
                f"Numeric Columns Imputed: {len(numeric_cols)}",
                f"Original Missing Values (Numeric): {initial_nan_count}",
                f"Remaining Missing Values (Numeric): {final_nan_count}",
                f"Output File: {output_file}"
            )
            send_progress("ğŸ‰ Imputation process finished.", 100)

            return {"ui": {"text": text_container_content},
                    "result": (str(output_file),)}

        except FileNotFoundError as fnf_e:
            error_msg = f"âŒ File Not Found Error: {str(fnf_e)}. Please check input file path."
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except ValueError as ve: # Catch specific errors like no numeric columns
            error_msg = f"âŒ Value Error during imputation: {str(ve)}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except Exception as e:
            error_msg = f"âŒ An unexpected error occurred during imputation: {str(e)}"
            import traceback
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}


class Descriptor_preprocessing_Classification:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_file": ("STRING",),
                "compounds_nan_threshold": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01, "display": "number"}),
                "descriptors_nan_threshold": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01, "display": "number"}),
                "imputation_method": (["mean", "median", "most_frequent"], {"default": "mean"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("INTEGRATED_PREPROCESSED_DATA",)
    FUNCTION = "preprocess"
    CATEGORY = "QSAR/CLASSIFICATION/PREPROCESSING"
    OUTPUT_NODE = True

    def preprocess(self, input_file, compounds_nan_threshold, descriptors_nan_threshold, imputation_method):
        send_progress("ğŸš€ Starting Integrated Descriptor Preprocessing...", 0)
        output_dir = "QSAR/Descriptor_Preprocessing"
        inf_file = None
        output_file = ""
        initial_rows, initial_cols = 0, 0
        rows_after_compound_filter, cols_after_descriptor_filter = 0, 0
        final_rows, final_cols = 0, 0
        removed_compounds_count = 0
        removed_descriptors_count = 0
        total_inf_count = 0
        initial_nan_count_impute = 0
        final_nan_count_impute = -1 # Flag for not imputed yet

        try:
            os.makedirs(output_dir, exist_ok=True)
            send_progress(f"ğŸ“‚ Output directory checked/created: {output_dir}", 2)

            send_progress(f"â³ Loading data from: {input_file}", 5)
            data = pd.read_csv(input_file)
            initial_rows, initial_cols = data.shape
            send_progress(f"   Data loaded ({initial_rows} rows, {initial_cols} columns).", 8)

            # --- 1. Replace inf with nan ---
            send_progress("â¡ï¸ Step 1: Replacing Inf values with NaN...", 10)
            numeric_df = data.select_dtypes(include=[np.number])
            inf_mask = numeric_df.isin([np.inf, -np.inf])
            inf_columns = numeric_df.columns[inf_mask.any(axis=0)].tolist()
            total_inf_count = inf_mask.sum().sum()

            if total_inf_count > 0:
                data.replace([np.inf, -np.inf], np.nan, inplace=True)
                send_progress(f"   Replaced {total_inf_count} Inf values in {len(inf_columns)} columns.", 15)
                # Save Inf report (optional, could be a parameter)
                try:
                    inf_counts = numeric_df[inf_columns].isin([np.inf, -np.inf]).sum().reset_index()
                    inf_counts.columns = ["Feature", "Original_Inf_Count"]
                    inf_counts = inf_counts[inf_counts["Original_Inf_Count"] > 0]
                    inf_file = os.path.join(output_dir, "integrated_inf_features_report.csv")
                    inf_counts.to_csv(inf_file, index=False)
                    send_progress(f"   Inf report saved: {inf_file}", 20)
                except Exception as report_e:
                    send_progress(f"   Warning: Could not save Inf report: {report_e}", 20)
                    inf_file = "Error saving report"
            else:
                 send_progress("   âœ… No Inf values found.", 20)
            send_progress("   Step 1 Complete.", 25)


            # --- 2. Remove compounds with high nan percentage ---
            send_progress(f"â¡ï¸ Step 2: Removing compounds with > {compounds_nan_threshold*100:.0f}% NaN...", 30)
            current_cols = data.shape[1] # Use current number of columns
            nan_counts_comp = data.isna().sum(axis=1)
            nan_percentage_comp = nan_counts_comp / current_cols
            data = data[nan_percentage_comp <= compounds_nan_threshold].copy() # Apply filter and copy
            rows_after_compound_filter = data.shape[0]
            removed_compounds_count = initial_rows - rows_after_compound_filter
            send_progress(f"   Removed {removed_compounds_count} compounds. Kept {rows_after_compound_filter}.", 45)
            send_progress("   Step 2 Complete.", 50)

            # --- 3. Remove descriptors with high nan percentage ---
            send_progress(f"â¡ï¸ Step 3: Removing descriptors with > {descriptors_nan_threshold*100:.0f}% NaN...", 55)
            nan_percentage_desc = data.isna().mean()
            retained_columns = nan_percentage_desc[nan_percentage_desc <= descriptors_nan_threshold].index.tolist()
            # Ensure essential columns
            essential_cols = ["Label", "SMILES", "Name"]
            for col in essential_cols:
                 if col in data.columns and col not in retained_columns:
                      retained_columns.append(col)
            data = data[retained_columns].copy() # Apply filter and copy
            cols_after_descriptor_filter = data.shape[1]
            removed_descriptors_count = current_cols - cols_after_descriptor_filter # Compare to cols *before* this step
            send_progress(f"   Removed {removed_descriptors_count} descriptors. Kept {cols_after_descriptor_filter}.", 70)
            send_progress("   Step 3 Complete.", 75)

            # --- 4. Impute remaining missing values ---
            send_progress(f"â¡ï¸ Step 4: Imputing remaining NaNs using '{imputation_method}'...", 80)
            non_numeric_cols = data.select_dtypes(exclude=[np.number]).columns.tolist()
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

            if numeric_cols:
                non_numeric_data = data[non_numeric_cols]
                numeric_data = data[numeric_cols]
                initial_nan_count_impute = numeric_data.isna().sum().sum()

                if initial_nan_count_impute > 0:
                    imputer = SimpleImputer(strategy=imputation_method)
                    imputed_numeric_data = pd.DataFrame(imputer.fit_transform(numeric_data), columns=numeric_cols, index=data.index)
                    final_nan_count_impute = imputed_numeric_data.isna().sum().sum()
                    # Recombine
                    data = pd.concat([non_numeric_data, imputed_numeric_data], axis=1)
                    data = data[retained_columns] # Keep column order
                    send_progress(f"   Imputed {initial_nan_count_impute} values. Remaining NaNs (numeric): {final_nan_count_impute}.", 88)
                else:
                    send_progress("   âœ… No missing values to impute in numeric columns.", 88)
                    final_nan_count_impute = 0 # Set to 0 if none needed
            else:
                 send_progress("   No numeric columns found for imputation.", 88)
                 final_nan_count_impute = 0 # Set to 0 if none needed

            send_progress("   Step 4 Complete.", 90)

            # Final shape
            final_rows, final_cols = data.shape

            # --- 5. Save final data ---
            send_progress("ğŸ’¾ Saving final preprocessed data...", 92)
            output_file = os.path.join(output_dir, "integrated_preprocessed_data.csv")
            data.to_csv(output_file, index=False)
            send_progress(f"   Preprocessed data saved to: {output_file}", 94)

            # --- 6. Generate Summary ---
            send_progress("ğŸ“ Generating final summary...", 95)
            text_container_content = create_text_container(
                "ğŸ”¹ **Integrated Descriptor Preprocessing Completed!** ğŸ”¹",
                f"Input File: {os.path.basename(input_file)} ({initial_rows} rows, {initial_cols} columns)",
                "--- Processing Steps ---",
                f"1. Inf Values Replaced: {total_inf_count} (Report: {inf_file if inf_file else 'N/A'})",
                f"2. Compounds Removed (> {compounds_nan_threshold*100:.0f}% NaN): {removed_compounds_count} (Kept: {rows_after_compound_filter})",
                f"3. Descriptors Removed (> {descriptors_nan_threshold*100:.0f}% NaN): {removed_descriptors_count} (Kept: {cols_after_descriptor_filter})",
                f"4. Imputation ('{imputation_method}'): {initial_nan_count_impute} values imputed. Remaining NaNs: {final_nan_count_impute}",
                "--- Final Output ---",
                f"Final Data Shape: {final_rows} rows, {final_cols} columns",
                f"Output File: {output_file}"
            )
            send_progress("ğŸ‰ Integrated preprocessing finished successfully!", 100)

            return {"ui": {"text": text_container_content},
                    "result": (str(output_file),)}

        except FileNotFoundError as fnf_e:
            error_msg = f"âŒ File Not Found Error: {str(fnf_e)}. Please check input file path."
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}
        except Exception as e:
            error_msg = f"âŒ An unexpected error occurred during integrated preprocessing: {str(e)}"
            import traceback
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
            send_progress(error_msg)
            return {"ui": {"text": create_text_container(error_msg)}, "result": (",")}


NODE_CLASS_MAPPINGS = {
    "Replace_inf_with_nan_Classification": Replace_inf_with_nan_Classification,
    "Remove_high_nan_compounds_Classification": Remove_high_nan_compounds_Classification,
    "Remove_high_nan_descriptors_Classification": Remove_high_nan_descriptors_Classification,
    "Impute_missing_values_Classification": Impute_missing_values_Classification,
    "Descriptor_preprocessing_Classification": Descriptor_preprocessing_Classification
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Replace_inf_with_nan_Classification": "Replace Inf with NaN (Classification)", # Updated
    "Remove_high_nan_compounds_Classification": "Remove High NaN Compounds (Classification)", # Updated
    "Remove_high_nan_descriptors_Classification": "Remove High NaN Descriptors (Classification)", # Updated
    "Impute_missing_values_Classification": "Impute Missing Values (Classification)", # Updated
    "Descriptor_preprocessing_Classification": "Descriptor Preprocessing (Integrated) (Classification)" # Updated
} 