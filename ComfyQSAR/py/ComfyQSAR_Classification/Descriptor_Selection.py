import os
import pandas as pd
import numpy as np
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.feature_selection import RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.preprocessing import StandardScaler
from .Data_Loader import create_text_container

class Feature_Selection_Classification:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_file": ("STRING",),
                "method": (["Lasso", "RandomForest", "DecisionTree", "XGBoost", "LightGBM", "RFE", "SelectFromModel"],),
                "advanced": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                # Model selection and basic parameters
                "model": (["Lasso", "RandomForest", "DecisionTree", "XGBoost", "LightGBM"],{"default": "None", "description": "Model to use with RFE or SelectFromModel methods"}),
                "target_column": ("STRING", {"default": "Label", "description": "Target column name in the dataset"}),
                "n_features": ("INT", {"default": 10, "min": 1, "max": 1000, "step": 1, "description": "Number of features to select"}),
                "threshold": ("STRING", {"default": "percentile(90)", "description": "Threshold for feature selection (percentile or absolute value)"}),
                
                # Lasso related parameters
                "alpha": ("FLOAT", {"default": 0.01, "min": 0.0001, "max": 10.0, "step": 0.001, "description": "Regularization strength for Lasso"}),
                "max_iter": ("INT", {"default": 1000, "min": 100, "max": 10000, "step": 100, "description": "Maximum number of iterations"}),
                
                # Tree-based model parameters
                "n_estimators": ("INT", {"default": 100, "min": 10, "max": 1000, "step": 10, "description": "Number of trees in ensemble models"}),
                "max_depth": ("INT", {"default": 5, "min": 1, "max": 50, "step": 1, "description": "Maximum depth of trees"}),
                "min_samples_split": ("INT", {"default": 2, "min": 2, "max": 20, "step": 1, "description": "Minimum samples required to split a node"}),
                "criterion": (["gini", "entropy"], {"description": "Function to measure quality of split"}),
                "learning_rate": ("FLOAT", {"default": 0.1, "min": 0.001, "max": 1.0, "step": 0.01, "description": "Learning rate for boosting models"}),
                
                # Additional parameters
                "n_iterations": ("INT", {"default": 100, "min": 10, "max": 1000, "step": 10, "description": "Number of iterations for stability analysis"}),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("OUTPUT_FILE",)
    FUNCTION = "select_features"
    CATEGORY = "QSAR/CLASSIFICATION/SELECTION"
    OUTPUT_NODE = True
    
    def select_features(self, input_file, advanced, target_column="Label", method="lasso", n_features=10, threshold="percentile(90)",
                      model=None, alpha=0.01, max_iter=1000, n_estimators=100, max_depth=5, min_samples_split=2, 
                      criterion="gini", learning_rate=0.1, n_iterations=100):
        """
        Feature selection supporting strategy-method and model-based approaches.
        """
        os.makedirs("QSAR/Selection", exist_ok=True)
        df = pd.read_csv(input_file)
        
        if target_column not in df.columns:
            raise ValueError(f"Target column '{target_column}' not found in the dataset.")

        X_raw = df.drop(columns=[target_column])
        y = df[target_column]

        # ë¬´í•œê°’ ë° NaN ì²˜ë¦¬
        print("ë°ì´í„° ì „ì²˜ë¦¬: ë¬´í•œê°’ ë° NaN ì²˜ë¦¬ ì¤‘...")
        X = X_raw.replace([np.inf, -np.inf], np.nan)
        
        # NaN ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
        for col in X.columns:
            if X[col].isnull().any():
                median_val = X[col].median()
                if np.isnan(median_val):  # ì—´ ì „ì²´ê°€ NaNì¸ ê²½ìš°
                    median_val = 0
                X[col] = X[col].fillna(median_val)
        
        # íŠ¹ìˆ˜ ê°’ ì²˜ë¦¬: ë§¤ìš° í° ê°’ì„ ê°ì§€í•˜ê³  ëŒ€ì²´
        for col in X.columns:
            # ë§¤ìš° í° ê°’(ì˜ˆ: float32 ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’) í™•ì¸
            if X[col].abs().max() > 1e30:
                # ë¬¸ì œê°€ ìˆëŠ” ì—´ì„ í‘œì¤€í™”í•˜ê±°ë‚˜ ê°’ ë²”ìœ„ ì œí•œ
                X[col] = np.clip(X[col], -1e30, 1e30)
                print(f"ì—´ '{col}'ì— ë§¤ìš° í° ê°’ì´ ìˆì–´ ë²”ìœ„ë¥¼ ì œí•œí–ˆìŠµë‹ˆë‹¤.")

        initial_feature_count = X.shape[1]

        model_abbr = None
        feature_importances = None

        def get_model(model_name):
            if model_name == "random_forest":
                return RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                          min_samples_split=min_samples_split, criterion=criterion), "RF"
            elif model_name == "decision_tree":
                return DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,
                                          criterion=criterion), "DT"
            elif model_name == "xgboost":
                return XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                 learning_rate=learning_rate, random_state=42, eval_metric="logloss"), "XGB"
            elif model_name == "lightgbm":
                return LGBMClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                  learning_rate=learning_rate, random_state=42), "LGBM"
            elif model_name == "lasso":
                return LogisticRegression(penalty='l1', solver='saga', C=1/alpha, max_iter=max_iter, random_state=42), "LASSO"
            else:
                raise ValueError(f"Invalid model name '{model_name}'.")

        try:
            if method == "lasso":
                model, model_abbr = get_model("lasso")
                
                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                model.fit(X_scaled, y)
                selected_columns = X.columns[model.coef_[0] != 0]
                X_new = X[selected_columns]

            elif method == "rfe":
                if model is None:
                    raise ValueError("'rfe' method requires a model.")
                
                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                model, model_abbr = get_model(model)
                print(f"RFE íŠ¹ì„± ì„ íƒ ì‹œì‘: ëª¨ë¸={model_abbr}, ì„ íƒí•  íŠ¹ì„± ìˆ˜={n_features}")
                
                try:
                    selector = RFE(estimator=model, n_features_to_select=n_features, step=0.05)
                    selector.fit(X_scaled, y)
                    selected_columns = X.columns[selector.get_support()]
                    X_new = X[selected_columns]
                    print(f"RFE íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {len(selected_columns)}ê°œ íŠ¹ì„± ì„ íƒë¨")
                except Exception as e:
                    print(f"RFE ì˜¤ë¥˜ ë°œìƒ: {e}")
                    print("ëŒ€ì²´ ì „ëµ ì‹œë„: SelectFromModel ì‚¬ìš©")
                    
                    # RFE ì‹¤íŒ¨ ì‹œ SelectFromModelë¡œ ëŒ€ì²´
                    model.fit(X_scaled, y)
                    if hasattr(model, "feature_importances_"):
                        feature_importances = model.feature_importances_
                    elif hasattr(model, "coef_"):
                        feature_importances = np.abs(model.coef_[0])
                    
                    # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ n_features ì„ íƒ
                    threshold = np.sort(feature_importances)[-min(n_features, len(feature_importances))]
                    selector = SelectFromModel(estimator=model, threshold=threshold, prefit=True)
                    selected_columns = X.columns[selector.get_support()]
                    X_new = X[selected_columns]
                    print(f"ëŒ€ì²´ ì „ëµ ì™„ë£Œ: {len(selected_columns)}ê°œ íŠ¹ì„± ì„ íƒë¨")

            elif method == "select_from_model":
                if model is None:
                    raise ValueError("'select_from_model' method requires a model.")
                
                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                model, model_abbr = get_model(model)
                model.fit(X_scaled, y)

                if hasattr(model, "feature_importances_"):
                    feature_importances = model.feature_importances_
                elif hasattr(model, "coef_"):
                    feature_importances = np.abs(model.coef_[0])
                else:
                    raise ValueError(f"The model {model} does not provide feature importances.")

                if isinstance(threshold, str) and "percentile" in threshold:
                    percentile_value = int(threshold.replace("percentile(", "").replace(")", ""))
                    threshold = np.percentile(feature_importances, percentile_value)

                selector = SelectFromModel(estimator=model, threshold=threshold, prefit=True)
                selected_columns = X.columns[selector.get_support()]
                X_new = X[selected_columns]

            elif method in ["random_forest", "decision_tree"]:
                feature_importance_matrix = np.zeros((n_iterations, X.shape[1]))

                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                for i in range(n_iterations):
                    model_iter, model_abbr = get_model(method)
                    model_iter.fit(X_scaled, y)
                    feature_importance_matrix[i] = model_iter.feature_importances_

                feature_importances = np.mean(feature_importance_matrix, axis=0)

                if isinstance(threshold, str) and "percentile" in threshold:
                    percentile_value = int(threshold.replace("percentile(", "").replace(")", ""))
                    threshold = np.percentile(feature_importances, percentile_value)
                else:
                    threshold = float(threshold)

                important_indices = np.where(feature_importances >= threshold)[0]
                selected_columns = X.columns[important_indices]
                X_new = X[selected_columns]

            elif method in ["xgboost", "lightgbm"]:
                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                model, model_abbr = get_model(method)
                model.fit(X_scaled, y)
                feature_importances = model.feature_importances_

                if isinstance(threshold, str) and "percentile" in threshold:
                    percentile_value = int(threshold.replace("percentile(", "").replace(")", ""))
                    threshold = np.percentile(feature_importances, percentile_value)
                else:
                    threshold = float(threshold)

                important_indices = np.where(feature_importances >= threshold)[0]
                selected_columns = X.columns[important_indices]
                X_new = X[selected_columns]

            else:
                raise ValueError(f"Unsupported method '{method}'")

        except Exception as e:
            error_msg = f"íŠ¹ì„± ì„ íƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(error_msg)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê°„ë‹¨í•œ ì „ëµìœ¼ë¡œ ëŒ€ì²´
            print("ì˜¤ë¥˜ë¡œ ì¸í•´ ê¸°ë³¸ ìƒê´€ê´€ê³„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            
            # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
            correlations = []
            for col in X.columns:
                try:
                    corr = abs(X[col].corr(pd.Series(y)))
                    if np.isnan(corr):
                        corr = 0
                    correlations.append((col, corr))
                except:
                    correlations.append((col, 0))
            
            # ìƒê´€ê´€ê³„ ê¸°ì¤€ ì •ë ¬
            correlations.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ n_features ì„ íƒ
            selected_columns = [item[0] for item in correlations[:min(n_features, len(correlations))]]
            X_new = X[selected_columns]
            model_abbr = "CORR"
            
            print(f"ìƒê´€ê´€ê³„ ê¸°ë°˜ ì„ íƒ ì™„ë£Œ: {len(selected_columns)}ê°œ íŠ¹ì„± ì„ íƒë¨")
            
        # ê²°ê³¼ ìƒì„±
        final_feature_count = len(selected_columns)
        removed_features = initial_feature_count - final_feature_count

        selected_features = X_new.copy()
        selected_features["Label"] = y.reset_index(drop=True)

        filename = f"features_{method}_{model_abbr}_{initial_feature_count}_{final_feature_count}.csv"
        output_file = os.path.join("QSAR/Selection", filename)
        selected_features.to_csv(output_file, index=False)

        text_container = create_text_container(
            "ğŸ”¹ Feature Selection Completed! ğŸ”¹",
            f"ğŸ“Œ Method: {method} ({model_abbr})",
            f"ğŸ“Š Initial Features: {initial_feature_count}",
            f"ğŸ“‰ Selected Features: {final_feature_count}",
            f"ğŸ—‘ï¸ Removed: {removed_features}",
        )

        return {
            "ui": {"text": text_container},
            "result": (str(output_file),)
        }

# ë…¸ë“œ ë“±ë¡
NODE_CLASS_MAPPINGS = {
    "Feature_Selection_Classification": Feature_Selection_Classification
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Feature_Selection_Classification": "Feature Selection(Classification)"
} 